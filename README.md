# Performance Metric Tufts Cluster Scripts

This repository contains scripts to run the tagger and vertex selection performance scripts

# Tagger Metric Scripts

## Pixel Analysis Metrix (combined)

These are the instructions to run a single job to analyze all of the tagger output. This is the simpler process.  The next section will cover steps to run each file individually.

* First, prepare a location in your home directory to house the tagger code.  clone the larlitecv repo (or the dllee_unified repo) and copy `$LARLITECV_BASEDIR/app/AnalyzeTagger` to this folder. The scripts are designed to reference this relative location.
* modify `make_good_taggerout_list.py` to point to the source larcv/larlite files and the tagger output larcv/larlite files.  Typically, you won't adjust the source files as the metric samples will be fixed.  You will want to point to the output folder though for the tagger output
* run `make_good_taggerout_list.py`. Note that it requires ROOT with python bindings. The cluster copy of ROOT 5 do not include these bindings, so we have to use the container.

      (host)$ module load singularity 
      (host)$ singularity shell /cluster/tufts/wongjiradlab/larbys/images/dllee_unified/singularity-dllee-unified-[date].img
      (container)> bash
      (container bash-shell)$ source /usr/local/bin/thisroot.sh
      (container bash-shell)$ python make_good_taggerout_list.py
      (container bash-shell)$ exit
      (container)> exit
      (host)$ 

* modify `AnalyzeTagger/pixana.cfg` to point to the input lists generated by the above script.
* run the tagger pixel analysis program on a single node. (Note: at this point you shoud be back to the host shell)

      $ sbatch submit_combined_pixana.cfg

  The output from the job should be directed to `slurmlog_combined_pixana.txt`. You can peak at it using

      $ tail -n 50 slurmlog_combined_pixana.cfg

  or check the job status using

      $ squeue -u [username]


## Pixel Analysis Metrix (separate job per file ID)

To save time, we can run the analysis script separately on each file. Then combine them as a final, separate step.

* First, prepare a location in your home directory to house the tagger code.  clone the larlitecv repo (or the dllee_unified repo) and copy `$LARLITECV_BASEDIR/app/AnalyzeTagger` to this folder. The scripts are designed to reference this relative location.
* modify `make_good_taggerout_list.py` to point to the source larcv/larlite files and the tagger output larcv/larlite files.  Typically, you won't adjust the source files as the metric samples will be fixed.  You will want to point to the output folder though for the tagger output
* run `make_good_taggerout_list.py`. Note that it requires ROOT with python bindings. The cluster copy of ROOT 5 do not include these bindings, so we have to use the container.

      (host)$ module load singularity 
      (host)$ singularity shell /cluster/tufts/wongjiradlab/larbys/images/dllee_unified/singularity-dllee-unified-[date].img
      (container)> bash
      (container bash-shell)$ source /usr/local/bin/thisroot.sh
      (container bash-shell)$ python make_good_taggerout_list.py
      (container bash-shell)$ exit
      (container)> exit
      (host)$ 

* 